{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization\n",
    " A central problem in machine learning is how to make an algorithm that will perform well not just on the training data, but also on new inputs. Many strategies used in machine learning are explicitly designed to reduce the test error, possibly at the expense of increased training error. These strategies are known collectively as regularization.\n",
    "\n",
    "Therefore regularization increases training error but reduces generalization error hence more no of epochs are needed to get the desired result.Regularization helps to reduce overfitting of the model.\n",
    "\n",
    "There are many regularization techniques used some but extra term in objective function and some but extra constraint on the model.\n",
    "\n",
    " 1. L1/L2 regularizers\n",
    " 2. DropOut\n",
    " 3. Data Augmentation\n",
    " 4. Label Smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1/L2 Regularizers\n",
    "L1 and L2 regularizers are some time known as weight decay.\n",
    "\n",
    "L1 Regularization works by adding an l1 norm to the cost function.\n",
    "\n",
    "L2 Regularization works by adding an l2 norm to the cost function. \n",
    "\n",
    "The idea behind l1 and l2 norm is smaller weight generalizes the model better so both of these norm perform some kind of weight decay.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L2 regularization \n",
    "    $$\n",
    "    C = any\\ loss\\ function  + \\frac{\\lambda}{2n}\\sum w^2\n",
    "    $$\n",
    "    \n",
    "Here λ is a regularization parameter and n is the size of training data w is the weight.we are adding a sum of squares of all weights to the cost function which is scaled by λ/2n where λ > 0.\n",
    "\n",
    "The intitution behind the l2 reguarization is if cost function is increased more weights will be penalize so the nettwork prefers to learn small weights.Large weights will only be allowed if they considerably improve the first part of the cost function. Put another way, regularization can be viewed as a way of compromising between finding small weights and minimizing the original cost function. The relative importance of the two elements of the compromise depends on the value of λ: when λ is small we prefer to minimize the original cost function, but when λ is large we prefer small weights.\n",
    "\n",
    "Updating weight formulae while backprop\n",
    "$$\n",
    "w = w - \\eta \\frac{\\partial C}{\\partial w} - \\frac {\\eta \\lambda} {n} w\n",
    "$$\n",
    "\n",
    "$$\n",
    "w = \\left( 1 - \\frac{\\eta \\lambda } {n} \\right) w - \\eta \\frac{\\partial C}{\\partial w} \n",
    "$$\n",
    "\n",
    "Here \n",
    "$$\n",
    "\\left( 1 - \\frac{\\eta \\lambda } {n} \\right)\n",
    "$$\n",
    "is the rescaling factor for weights or the weight decay factor.For very small λ value it is allowing big weights and if λ value is big it is penealizing the weights.\n",
    "\n",
    "Why is this going on? Heuristically, if the cost function is unregularized, then the length of the weight vector is likely to grow, all other things being equal. Over time this can lead to the weight vector being very large indeed. This can cause the weight vector to get stuck pointing in more or less the same direction, since changes due to gradient descent only make tiny changes to the direction, when the length is long. I believe this phenomenon is making it hard for our learning algorithm to properly explore the weight space, and consequently harder to find good minima of the cost function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L1 regularization \n",
    "$$\n",
    "C = any\\ loss\\ function  + \\frac{\\lambda}{n}\\sum_w |w|\n",
    "$$\n",
    "\n",
    "L1 regularization is similar to l2 just the norm formulae changes from sum of squares to absolute value.\n",
    "\n",
    "Updating weight formulae while backprop\n",
    "$$\n",
    "w = w - \\eta \\frac{\\partial C}{\\partial w} - \\frac {\\eta \\lambda} {n} sign(w)\n",
    "$$\n",
    "\n",
    "sign(w) is just the sign of the weight vector +1 for positive weights and -1 for negative weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing L1 and L2 \n",
    "In both expressions the effect of regularization is to shrink the weights. This accords with our intuition that both kinds of regularization penalize large weights. But the way the weights shrink is different. In L1 regularization, the weights shrink by a constant amount toward 0. In L2 regularization, the weights shrink by an amount which is proportional to w. And so when a particular weight has a large magnitude, |w|, L1 regularization shrinks the weight much less than L2 regularization does. By contrast, when |w| is small, L1 regularization shrinks the weight much more than L2 regularization. The net result is that L1 regularization tends to concentrate the weight of the network in a relatively small number of high-importance connections, while the other weights are driven toward zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout \n",
    "Dropout is another regularization techniques which is very simple to understand.\n",
    "![](dropout.gif)\n",
    "So it takes a probability p and based on the value of p it randomly disables that percentage of neuron.\n",
    "\n",
    "For example if the dropout value is 0.3 on a layer. It will disable 30% neuron in the layer i.e zero the value of those neuron.\n",
    "\n",
    "While training with every batch a different set on neurons are disabled which is completely random.\n",
    "\n",
    "So why does dropout increases the robustness of the model?\n",
    "Heuristically, when we dropout different sets of neurons, it's rather like we're training different neural networks. And so the dropout procedure is like averaging the effects of a very large number of different networks. The different networks will overfit in different ways, and so, hopefully, the net effect of dropout will be to reduce overfitting.\n",
    "\n",
    "For example In cnn if the model is trained on dogs vs cats example and few particular neurons having higher weight, everytime the model witnesses the whiskers in the image it activates those neurons and we get cat. But what if those whiskers are no there then model fails significantly. so dropout forces the model to learn more attributes of the training data while training. \n",
    "\n",
    "when p = 0.5\n",
    "\n",
    "By repeating dropout over and over, our network will learn a set of weights and biases. Of course, those weights and biases will have been learnt under conditions in which half the hidden neurons were dropped out. When we actually run the full network that means that twice as many hidden neurons will be active. To compensate for that, we halve the weights outgoing from the hidden neurons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation\n",
    "The best way to make a machine learning model generalize better is to train it on more data. Of course, in practice, the amount of data we have is limited. One way to get around this problem is to create fake data and add it to the training set.\n",
    "Here Data Augmentation comes to rescue which is kind of regularization technique because now we have generated more data for the model where the image changes every time so model has to learn different attributes of the data which will lead to better generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization\n",
    "1. Eip session2\n",
    "2. Batch Norm\n",
    "3. Weight Norm\n",
    "3. Layer Norm\n",
    "https://mlexplained.com/2018/01/13/weight-normalization-and-layer-normalization-explained-normalization-in-deep-learning-part-2/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
